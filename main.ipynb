{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\zjuwa\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.738 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from loader import load_sentences, update_tag_scheme, load_or_create_maps\n",
    "from loader import char_mapping, tag_mapping, augment_with_pretrained\n",
    "from loader import prepare_dataset\n",
    "\n",
    "from model import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flags = tf.app.flags\n",
    "# flags.DEFINE_string(\"train_file\",   os.path.join(\"../CEC-Corpus/news.train\"),  \"Path for train data\")\n",
    "# flags.DEFINE_string(\"dev_file\",     os.path.join(\"../CEC-Corpus/news.dev\"),    \"Path for dev data\")\n",
    "# flags.DEFINE_string(\"test_file\",    os.path.join(\"../CEC-Corpus/news.test\"),   \"Path for test data\")\n",
    "# FLAGS = tf.app.flags.FLAGS\n",
    "config = {}\n",
    "\n",
    "config['crf'] = True # \"Use CRF\"\n",
    "config['clean'] = False # \"clean train folder\"\n",
    "config['train'] = True # \"weither train the model\"\n",
    "\n",
    "config['train_file'] = os.path.join(\"../CEC-Corpus/news.train\")\n",
    "config['dev_file'] = os.path.join(\"../CEC-Corpus/news.dev\")\n",
    "config['test_file'] = os.path.join(\"../CEC-Corpus/news.test\")\n",
    "\n",
    "config['map_file'] = 'maps.pkl' # \"file for maps\"\n",
    "config['emb_file'] = 'wiki_100.utf8' # \"Path for pre_trained embedding\"\n",
    "\n",
    "config['pre_emb'] = True # \"Wither use pre-trained embedding\"\n",
    "config['zeros'] = False  # \"Wither replace digits with zero\"\n",
    "config['lower'] = True   # \"Wither lower case\"\n",
    "\n",
    "config['seg_dim'] = 20 # \"Embedding size for segmentation, 0 if not used\"\n",
    "config['char_dim'] = 100 # \"Embedding size for characters\"\n",
    "config['lstm_dim'] = 200 # \"Num of hidden units in LSTM\"\n",
    "config['tag_schema'] = 'iobes' # \"tagging schema iobes or iob\"\n",
    "\n",
    "config['lr'] = 0.001 # \"Initial learning rate\"\n",
    "config['batch_size'] = 64 # \"Batch size\"\n",
    "config['optimizer'] = 'adam' # \"Optimizer for training\"\n",
    "config['clip'] = 5 # \"Gradient clip\"\n",
    "config['dropout'] = 0.5 # \"Dropout rate\"\n",
    "\n",
    "# load sentences and update them to format we want\n",
    "train_sentences = load_sentences(config['train_file'], config['lower'], config['zeros'])\n",
    "dev_sentences = load_sentences(config['dev_file'], config['lower'], config['zeros'])\n",
    "test_sentences = load_sentences(config['test_file'], config['lower'], config['zeros'])\n",
    "update_tag_scheme(train_sentences, config['tag_schema'])\n",
    "update_tag_scheme(test_sentences, config['tag_schema'])\n",
    "# load or create maps\n",
    "char_to_id, id_to_char, tag_to_id, id_to_tag = load_or_create_maps(train_sentences, test_sentences, config)\n",
    "\n",
    "config['num_tags'] = len(tag_to_id)\n",
    "config['num_chars'] = len(char_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2977 / 0 / 1488 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "# prepare data, get a collection of list containing index\n",
    "# data = [[chars], [idx_chars], [segments], [idx_tag]]\n",
    "# where segments is {0: word with one sigle char, 1: begin of a word, 2: inside a word, 3: end of a word}\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, char_to_id, tag_to_id, config['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, char_to_id, tag_to_id, config['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, char_to_id, tag_to_id, config['lower']\n",
    ")\n",
    "print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "    len(train_data), 0, len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# make path for store log and model if not exist\n",
    "# make_path(FLAGS)\n",
    "# if os.path.isfile(FLAGS.config_file):\n",
    "#     config = load_config(FLAGS.config_file)\n",
    "# else:\n",
    "#     config = config_model(char_to_id, tag_to_id)\n",
    "#     save_config(config, FLAGS.config_file)\n",
    "# make_path(FLAGS)\n",
    "\n",
    "# log_path = os.path.join(\"log\", FLAGS.log_file)\n",
    "# logger = get_logger(log_path)\n",
    "# print_config(config, logger)\n",
    "\n",
    "# limit GPU memory\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "model = Model(config)\n",
    "model.set_dataset(train_data, 'train_data')\n",
    "model.set_dataset(test_data, 'test_data')\n",
    "model.set_dataset(dev_data, 'dev_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "train_init = model.make_dataset_init('train_data', shuffle=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "step:100 loss:23.867287\n",
      "2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-050695b4a597>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                 \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                 \u001b[0mn_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace\\EventExtraction\\model.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(self, sess, is_train)\u001b[0m\n\u001b[0;32m    254\u001b[0m             global_step, loss, _ = sess.run(\n\u001b[0;32m    255\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m    257\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf_config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(100):\n",
    "        sess.run(train_init)\n",
    "        loss = []\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                step, batch_loss = model.run_step(sess, True)\n",
    "                loss.append(batch_loss)\n",
    "                n_batches += 1\n",
    "                if step % 100 == 0:\n",
    "                    print(\"step:{} loss:{:>9.6f}\".format(\n",
    "                         step, np.mean(loss)))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        print(i)\n",
    "\n",
    "# with tf.Session(config=tf_config) as sess:\n",
    "#     model = create_model(sess, Model, FLAGS.ckpt_path, load_word2vec, config, id_to_char, logger)\n",
    "#     logger.info(\"start training\")\n",
    "#     loss = []\n",
    "#     for i in range(100):\n",
    "#         for batch in train_manager.iter_batch(shuffle=True):\n",
    "#             step, batch_loss = model.run_step(sess, True, batch)\n",
    "#             loss.append(batch_loss)\n",
    "#             if step % FLAGS.steps_check == 0:\n",
    "#                 iteration = step // steps_per_epoch + 1\n",
    "#                 logger.info(\"iteration:{} step:{}/{}, \"\n",
    "#                             \"NER loss:{:>9.6f}\".format(\n",
    "#                     iteration, step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "#                 loss = []\n",
    "\n",
    "#         best = evaluate(sess, model, \"dev\", dev_manager, id_to_tag, logger, reset_best_socre=False)\n",
    "#         if best:\n",
    "#             save_model(sess, model, FLAGS.ckpt_path, logger)\n",
    "#         evaluate(sess, model, \"test\", test_manager, id_to_tag, logger, reset_best_socre=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
