{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from loader import load_sentences, update_tag_scheme, load_or_create_maps\n",
    "from loader import char_mapping, tag_mapping, augment_with_pretrained\n",
    "from loader import prepare_dataset\n",
    "from logger import get_logger\n",
    "\n",
    "from utils import clean, make_path, save_config, load_config\n",
    "from data_utils import load_word2vec\n",
    "\n",
    "from model import Model\n",
    "\n",
    "def estimate_accuracy(results):\n",
    "    n_total = 0\n",
    "    n_right = 0\n",
    "    for sent in results:\n",
    "        n = len(sent[0])\n",
    "        n_total += n\n",
    "        for i in range(n):\n",
    "            if sent[1][i] == sent[2][i]:\n",
    "                n_right += 1\n",
    "    return n_right/n_total\n",
    "\n",
    "def update_best_score(sess, model, score, dataset='dev_data'):\n",
    "    model_best = model.best_dev_score if dataset=='dev_data' else model.best_test_score\n",
    "    best_core = model_best.eval()\n",
    "    if best_core < score:\n",
    "        tf.assign(model_best, score).eval()\n",
    "    return best_core < score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['crf'] = True # \"Use CRF\"\n",
    "config['clean'] = False # \"clean train folder\"\n",
    "config['train'] = True # \"weither train the model\"\n",
    "\n",
    "config['train_file'] = os.path.join(\"../CEC-Corpus/news.train\")\n",
    "config['dev_file'] = os.path.join(\"../CEC-Corpus/news.dev\")\n",
    "config['test_file'] = os.path.join(\"../CEC-Corpus/news.test\")\n",
    "\n",
    "config['map_file'] = 'maps.pkl' # \"file for maps\"\n",
    "config['emb_file'] = 'wiki_100.utf8' # \"Path for pre_trained embedding\"\n",
    "\n",
    "config['ckpt_path'] = 'ckpt' # \"Path to save model\"\n",
    "config['result_path'] = 'result'\n",
    "config['config_file'] = 'config_file'\n",
    "config['log_file'] = 'train.log' # \"File for log\"\n",
    "\n",
    "config['pre_emb'] = True # \"Wither use pre-trained embedding\"\n",
    "config['zeros'] = False  # \"Wither replace digits with zero\"\n",
    "config['lower'] = True   # \"Wither lower case\"\n",
    "\n",
    "config['seg_dim'] = 20 # \"Embedding size for segmentation, 0 if not used\"\n",
    "config['char_dim'] = 100 # \"Embedding size for characters\"\n",
    "config['lstm_dim'] = 200 # \"Num of hidden units in LSTM\"\n",
    "config['tag_schema'] = 'iobes' # \"tagging schema iobes or iob\"\n",
    "\n",
    "config['lr'] = 0.001 # \"Initial learning rate\"\n",
    "config['batch_size'] = 64 # \"Batch size\"\n",
    "config['optimizer'] = 'adam' # \"Optimizer for training\"\n",
    "config['clip'] = 5 # \"Gradient clip\"\n",
    "config['dropout'] = 0.5 # \"Dropout rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2977 / 1489 / 1488 sentences in train / dev / test.\n"
     ]
    }
   ],
   "source": [
    "# load sentences and update them to format we want\n",
    "train_sentences = load_sentences(config['train_file'], config['lower'], config['zeros'])\n",
    "dev_sentences = load_sentences(config['dev_file'], config['lower'], config['zeros'])\n",
    "test_sentences = load_sentences(config['test_file'], config['lower'], config['zeros'])\n",
    "update_tag_scheme(train_sentences, config['tag_schema'])\n",
    "update_tag_scheme(test_sentences, config['tag_schema'])\n",
    "# load or create maps\n",
    "char_to_id, id_to_char, tag_to_id, id_to_tag = load_or_create_maps(train_sentences, test_sentences, config)\n",
    "\n",
    "# prepare data, get a collection of list containing index\n",
    "# data = [[chars], [idx_chars], [segments], [idx_tag]]\n",
    "# where segments is {0: word with one sigle char, 1: begin of a word, 2: inside a word, 3: end of a word}\n",
    "train_data = prepare_dataset(\n",
    "    train_sentences, char_to_id, tag_to_id, config['lower']\n",
    ")\n",
    "dev_data = prepare_dataset(\n",
    "    dev_sentences, char_to_id, tag_to_id, config['lower']\n",
    ")\n",
    "test_data = prepare_dataset(\n",
    "    test_sentences, char_to_id, tag_to_id, config['lower']\n",
    ")\n",
    "print(\"%i / %i / %i sentences in train / dev / test.\" % (\n",
    "    len(train_data), len(dev_data), len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare before running the model\n",
    "- load and save exist configuration\n",
    "- initialize the logger\n",
    "- set tf_config\n",
    "- set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 23:48:51,718 - train.log - INFO - crf            :\tTrue\n",
      "2018-03-17 23:48:51,718 - train.log - INFO - clean          :\tFalse\n",
      "2018-03-17 23:48:51,718 - train.log - INFO - train          :\tTrue\n",
      "2018-03-17 23:48:51,718 - train.log - INFO - train_file     :\t../CEC-Corpus/news.train\n",
      "2018-03-17 23:48:51,718 - train.log - INFO - dev_file       :\t../CEC-Corpus/news.dev\n",
      "2018-03-17 23:48:51,718 - train.log - INFO - test_file      :\t../CEC-Corpus/news.test\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - map_file       :\tmaps.pkl\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - emb_file       :\twiki_100.utf8\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - ckpt_path      :\tckpt\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - result_path    :\tresult\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - config_file    :\tconfig_file\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - log_file       :\ttrain.log\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - pre_emb        :\tTrue\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - zeros          :\tFalse\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - lower          :\tTrue\n",
      "2018-03-17 23:48:51,734 - train.log - INFO - seg_dim        :\t20\n",
      "2018-03-17 23:48:51,749 - train.log - INFO - char_dim       :\t100\n",
      "2018-03-17 23:48:51,749 - train.log - INFO - lstm_dim       :\t200\n",
      "2018-03-17 23:48:51,749 - train.log - INFO - tag_schema     :\tiobes\n",
      "2018-03-17 23:48:51,749 - train.log - INFO - lr             :\t0.001\n",
      "2018-03-17 23:48:51,749 - train.log - INFO - batch_size     :\t64\n",
      "2018-03-17 23:48:51,749 - train.log - INFO - optimizer      :\tadam\n",
      "2018-03-17 23:48:51,765 - train.log - INFO - clip           :\t5\n",
      "2018-03-17 23:48:51,766 - train.log - INFO - dropout        :\t0.5\n",
      "2018-03-17 23:48:51,768 - train.log - INFO - num_tags       :\t21\n",
      "2018-03-17 23:48:51,770 - train.log - INFO - num_chars      :\t2063\n",
      "c:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "# make path for store log and model if not exist\n",
    "make_path(config)\n",
    "if os.path.isfile(config['config_file']):\n",
    "    config = load_config(config['config_file'])\n",
    "else:\n",
    "    config['num_chars'] = len(char_to_id)\n",
    "    config['num_tags'] = len(tag_to_id)\n",
    "    save_config(config, config['config_file'])\n",
    "make_path(config)\n",
    "\n",
    "log_path = os.path.join(\"log\", config['log_file'])\n",
    "mylogger = get_logger(config['log_file'])\n",
    "# print config\n",
    "for k, v in config.items():\n",
    "    mylogger.info(\"{}:\\t{}\".format(k.ljust(15), v))\n",
    "\n",
    "# limit GPU memory\n",
    "tf_config = tf.ConfigProto()\n",
    "tf_config.gpu_options.allow_growth = False\n",
    "model = Model(config)\n",
    "model.set_dataset(train_data, 'train_data')\n",
    "model.set_dataset(test_data, 'test_data')\n",
    "model.set_dataset(dev_data, 'dev_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **TRAIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 00:03:33,223 - train.log - INFO - Reading model parameters from ckpt\\ee.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt\\ee.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-18 00:03:43,876 - train.log - INFO - iteration:0 step:2200 loss: 0.039448\n",
      "2018-03-18 00:03:46,193 - train.log - INFO - iteration0 finished.\n",
      "2018-03-18 00:03:46,193 - train.log - INFO - evaluate:test\n",
      "2018-03-18 00:03:49,643 - train.log - INFO - Accuracy: 0.7673322521792013.\n",
      "2018-03-18 00:03:49,644 - train.log - INFO - evaluate:dev\n",
      "2018-03-18 00:03:53,021 - train.log - INFO - Accuracy: 0.7814538676607642.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-9a867ac61c10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 \u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0mn_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\workspace\\EventExtraction\\model.py\u001b[0m in \u001b[0;36mrun_step\u001b[1;34m(self, sess, is_train)\u001b[0m\n\u001b[0;32m    276\u001b[0m             global_step, loss, _ = sess.run(\n\u001b[0;32m    277\u001b[0m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 278\u001b[1;33m                 feed_dict)\n\u001b[0m\u001b[0;32m    279\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    280\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\zjuwa\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf_config) as sess:\n",
    "    model.load(sess, config['ckpt_path'], load_word2vec, config, id_to_char, mylogger)\n",
    "    \n",
    "    train_init = model.make_dataset_init('train_data', shuffle=3000)\n",
    "    \n",
    "    for i in range(100):\n",
    "        sess.run(train_init)\n",
    "        loss = []\n",
    "        n_batches = 0\n",
    "        try:\n",
    "            while True:\n",
    "                step, batch_loss = model.run_step(sess, True)\n",
    "                loss.append(batch_loss)\n",
    "                n_batches += 1\n",
    "                if step % 100 == 0:\n",
    "                    mylogger.info(\"iteration:{} step:{} loss:{:>9.6f}\".format(\n",
    "                        i, step, np.mean(loss)))\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            pass\n",
    "        \n",
    "        mylogger.info(\"iteration{} finished.\".format(i))\n",
    "        \n",
    "        mylogger.info(\"evaluate:test\")\n",
    "        score = estimate_accuracy(model.evaluate(sess, 'test_data', id_to_tag, id_to_char))\n",
    "        isbest = update_best_score(sess, model, score, dataset='test_data')\n",
    "        mylogger.info(\"Accuracy: {}.\".format(score))\n",
    "        \n",
    "        mylogger.info(\"evaluate:dev\")\n",
    "        score = estimate_accuracy(model.evaluate(sess, 'dev_data', id_to_tag, id_to_char))\n",
    "        isbest = update_best_score(sess, model, score, dataset='dev_data')\n",
    "        mylogger.info(\"Accuracy: {}.\".format(score))\n",
    "        if isbest:\n",
    "            mylogger.info(\"New record, save current model.\")\n",
    "            model.save(sess, config['ckpt_path']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-17 23:53:24,072 - train.log - INFO - Reading model parameters from ckpt\\ee.ckpt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ckpt\\ee.ckpt\n",
      "0.78248936\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=tf_config) as sess:\n",
    "    model.load(sess, config['ckpt_path'], load_word2vec, config, id_to_char, mylogger)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
